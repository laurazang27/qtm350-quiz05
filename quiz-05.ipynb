{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f070b06",
   "metadata": {},
   "source": [
    "## Quiz 05 - Parallel Computing, Reproducibility, and Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6142c4",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "This quiz is based on the material covered in lectures 20 to 24. You may use\n",
    "any resources available to you, including the lecture notes and the internet.\n",
    "\n",
    "All the data required for this quiz can be found in the `data` folder within this repository. If you need to recreate the datasets, you can do so by running the Python script included in the `script-data-generation` folder.\n",
    "\n",
    "**Important:** Please start by completing Question 01 to set up the correct Python environment before proceeding with the other questions.\n",
    "\n",
    "This notebook contains the questions you need to answer.\n",
    "If possible, please submit your answers as an `.html` file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47fa52",
   "metadata": {},
   "source": [
    "### **Question 01: Setting up the Python Environment**\n",
    "\n",
    "Before proceeding with the rest of the quiz, it is important to set up a Python environment with specific package versions to ensure compatibility and reproducibility. This quiz requires **Python 3.10** and the following packages with exact versions:\n",
    "- `dask-sql=2024.5.0`\n",
    "- `dask=2024.4.1`\n",
    "- `ipykernel=6.29.3`\n",
    "- `joblib=1.3.2`\n",
    "- `numpy=1.26.4`\n",
    "- `pandas=2.2.1`\n",
    "\n",
    "You can use tools like `conda`, `pipenv`, or `uv` to manage your environment. If you use conda (recommended), please make sure you **create the environment and install all packages in the same command**. Also include `-c conda-forge` in your command. Make sure to change your current environment to the new environment after creation. \n",
    "\n",
    "Write the terminal commands in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb895df",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n quiz-env python=3.10 \\\n",
    "    dask-sql=2024.5.0 \\\n",
    "    dask=2024.4.1 \\\n",
    "    ipykernel=6.29.3 \\\n",
    "    joblib=1.3.2 \\\n",
    "    numpy=1.26.4 \\\n",
    "    pandas=2.2.1 \\\n",
    "    -c conda-forge -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8e018",
   "metadata": {},
   "source": [
    "### Question 02: Understanding the `map` Function and Parallelism\n",
    "\n",
    "The built-in Python `map()` function applies a function to each element sequentially. Using `joblib`, rewrite the following serial code to run in parallel using **all available cores** (hint: use `n_jobs=-1`). Compare the results to verify correctness.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def cube_root(x):\n",
    "    return x ** (1/3)\n",
    "\n",
    "numbers = np.arange(1, 500001)\n",
    "\n",
    "# Serial version using map\n",
    "serial_result = list(map(cube_root, numbers))\n",
    "print(\"First 5 serial results:\", serial_result[:5])\n",
    "```\n",
    "\n",
    "Write the parallel version using `joblib.Parallel` and `delayed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd4ff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 serial results: [np.float64(1.0), np.float64(1.2599210498948732), np.float64(1.4422495703074083), np.float64(1.5874010519681994), np.float64(1.7099759466766968)]\n",
      "First 5 parallel results: [np.float64(1.0), np.float64(1.2599210498948732), np.float64(1.4422495703074083), np.float64(1.5874010519681994), np.float64(1.7099759466766968)]\n",
      "Results match serial version: True\n"
     ]
    }
   ],
   "source": [
    "# Please write your answer here.\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def cube_root(x):\n",
    "    return x ** (1/3)\n",
    "\n",
    "numbers = np.arange(1, 500001)\n",
    "\n",
    "# Serial version using map\n",
    "serial_result = list(map(cube_root, numbers))\n",
    "print(\"First 5 serial results:\", serial_result[:5])\n",
    "\n",
    "# Parallel version using joblib\n",
    "parallel_result = Parallel(n_jobs=-1)(\n",
    "    delayed(cube_root)(x) for x in numbers\n",
    ")\n",
    "\n",
    "print(\"First 5 parallel results:\", parallel_result[:5])\n",
    "\n",
    "# Verify correctness\n",
    "print(\"Results match serial version:\", serial_result == parallel_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94ca03",
   "metadata": {},
   "source": [
    "### Question 03: Measuring Parallel Speedup\n",
    "\n",
    "Create a function called `simulate_computation` that generates 100,000 random numbers and calculates their variance. Using `%timeit`, measure and compare the execution time of:\n",
    "\n",
    "1. Running the function **4 times sequentially** in a list comprehension (`[simulate_computation() for _ in range(4)]`)\n",
    "2. Running the function **4 times in parallel** using `joblib` with 4 workers\n",
    "\n",
    "Print and compare both timing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27f87d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential execution:\n",
      "1.52 ms ± 86.2 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Parallel execution:\n",
      "12.8 ms ± 806 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Please write your answer here.\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Define the computation function\n",
    "def simulate_computation():\n",
    "    data = np.random.rand(100_000)\n",
    "    return np.var(data)\n",
    "\n",
    "# Sequential execution (4 times)\n",
    "print(\"Sequential execution:\")\n",
    "%timeit [simulate_computation() for _ in range(4)]\n",
    "\n",
    "\n",
    "# Parallel execution using joblib (4 workers)\n",
    "\n",
    "print(\"\\nParallel execution:\")\n",
    "%timeit Parallel(n_jobs=4)(delayed(simulate_computation)() for _ in range(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47889840",
   "metadata": {},
   "source": [
    "### Question 04: Dask Array with Custom Chunk Sizes\n",
    "\n",
    "Create a Dask array of shape (5000, 2000) filled with random integers between 1 and 100. Use chunks of size (500, 500). Then:\n",
    "\n",
    "1. Compute the sum of each row\n",
    "2. Calculate the mean and standard deviation of the entire array\n",
    "3. Print all three results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9c0c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement dask-array (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for dask-array\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eaa4a87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'numpy'\n\nDask array requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                 # either conda install\n  python -m pip install \"dask[array]\" --upgrade  # or python -m pip install",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/qtm350-quiz05/.conda/lib/python3.10/site-packages/dask/array/__init__.py:333\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bool_ \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    335\u001b[0m         complex64,\n\u001b[1;32m    336\u001b[0m         complex128,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m         uint64,\n\u001b[1;32m    353\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Please write your answer here.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mda\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create Dask array: shape (5000, 2000), ints 1–100, chunks of (500, 500)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m da\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\n\u001b[1;32m      6\u001b[0m     low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m101\u001b[39m,\n\u001b[1;32m      8\u001b[0m     size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5000\u001b[39m, \u001b[38;5;241m2000\u001b[39m),\n\u001b[1;32m      9\u001b[0m     chunks\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/qtm350-quiz05/.conda/lib/python3.10/site-packages/dask/array/__init__.py:632\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    626\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDask array requirements are not installed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease either conda or pip install as follows:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  conda install dask                 # either conda install\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  python -m pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdask[array]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m --upgrade  # or python -m pip install\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    631\u001b[0m     )\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _array_expr_enabled():\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_not_implemented_error\u001b[39m(attr_name):\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'numpy'\n\nDask array requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                 # either conda install\n  python -m pip install \"dask[array]\" --upgrade  # or python -m pip install"
     ]
    }
   ],
   "source": [
    "# Please write your answer here.\n",
    "import dask.array as da\n",
    "\n",
    "# Create Dask array: shape (5000, 2000), ints 1–100, chunks of (500, 500)\n",
    "x = da.random.randint(\n",
    "    low=1,\n",
    "    high=101,\n",
    "    size=(5000, 2000),\n",
    "    chunks=(500, 500)\n",
    ")\n",
    "\n",
    "# 1. Compute the sum of each row\n",
    "row_sums = x.sum(axis=1).compute()\n",
    "\n",
    "# 2. Mean and standard deviation of the entire array\n",
    "mean_val = x.mean().compute()\n",
    "std_val  = x.std().compute()\n",
    "\n",
    "# 3. Print all three results\n",
    "print(\"Row sums (first 10):\", row_sums[:10])\n",
    "print(\"Mean of entire array:\", mean_val)\n",
    "print(\"Std of entire array:\", std_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential execution:\n",
      "1.52 ms ± 86.2 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Parallel execution:\n",
      "12.8 ms ± 806 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Please write your answer here.\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Define the computation function\n",
    "def simulate_computation():\n",
    "    data = np.random.rand(100_000)\n",
    "    return np.var(data)\n",
    "\n",
    "# Sequential execution (4 times)\n",
    "print(\"Sequential execution:\")\n",
    "%timeit [simulate_computation() for _ in range(4)]\n",
    "\n",
    "\n",
    "# Parallel execution using joblib (4 workers)\n",
    "\n",
    "print(\"\\nParallel execution:\")\n",
    "%timeit Parallel(n_jobs=4)(delayed(simulate_computation)() for _ in range(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be770baf",
   "metadata": {},
   "source": [
    "### Question 05: Optimising Chunk Size\n",
    "\n",
    "The chunk size significantly affects Dask performance. Create a Dask array with 100,000 random numbers and test three different chunk sizes: 1,000 (many small chunks), 10,000 (medium chunks), and 50,000 (few large chunks).\n",
    "\n",
    "For each configuration, measure the time to compute `mean(sin(x) + cos(x))`. Which chunk size performed best? Explain why in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc038e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer here.\n",
    "\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "chunk_sizes = [1_000, 10_000, 50_000]\n",
    "timings = {}\n",
    "\n",
    "for chunk in chunk_sizes:\n",
    "    # Create a Dask array with given chunk size\n",
    "    x = da.random.random(100_000, chunks=chunk)\n",
    "    expr = da.sin(x) + da.cos(x)\n",
    "    \n",
    "    # Time the computation of mean(sin(x) + cos(x))\n",
    "    start = time.perf_counter()\n",
    "    result = expr.mean().compute()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    elapsed = end - start\n",
    "    timings[chunk] = elapsed\n",
    "    print(f\"Chunk size {chunk:>6}: mean={result:.6f}, time={elapsed:.4f} seconds\")\n",
    "\n",
    "# Brief comment\n",
    "best_chunk = min(timings, key=timings.get)\n",
    "print(f\"\\nThe best-performing chunk size in my run was {best_chunk},\")\n",
    "print(\"because it balances overhead (too many tiny chunks) and lack of parallelism (too few huge chunks).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620cfde",
   "metadata": {},
   "source": [
    "### Question 06: Reading Parquet Files with Column Selection\n",
    "\n",
    "The `data` folder contains Parquet files for multiple countries. Using Dask, read **all Parquet files at once** (`data/*.parquet`), but load only the `year` and `population` columns.\n",
    "\n",
    "Calculate the total world population for each year across all countries and display the results sorted by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c5b75",
   "metadata": {},
   "source": [
    "### Question 07: Dask SQL with Multiple Conditions\n",
    "\n",
    "Load the `data.csv` file into a Dask DataFrame and register it as a SQL table. Write a SQL query that:\n",
    "\n",
    "1. Selects countries where `gdp_per_capita` was between 10000 and 50000\n",
    "2. Filters for years between 2000 and 2020\n",
    "3. Orders results by `gdp_per_capita` in descending order\n",
    "4. Limits to the top 2 results\n",
    "\n",
    "Execute the query and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f40a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "### Question 08: Dask SQL with Aggregation\n",
    "\n",
    "Using the same `data.csv` file, write a SQL query that calculates:\n",
    "\n",
    "1. The average GDP per capita for each country\n",
    "2. The minimum and maximum years in the dataset for each country\n",
    "\n",
    "Group by country and display all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d22dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "### Question 09: Generating `requirements.txt` and `environment.yml` Files\n",
    "\n",
    "Write the commands to:\n",
    "\n",
    "1. Export your current environment's packages to a `requirements.txt` and an `environment.yml` file\n",
    "2. Show how someone else would install these exact dependencies in these two cases\n",
    "\n",
    "Explain each step with comments. It is not necessary to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fea278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5438c",
   "metadata": {},
   "source": [
    "### Question 10: Troubleshooting a Broken Dockerfile\n",
    "\n",
    "The following Dockerfile has several errors. Identify and fix 5 issues, then explain what was wrong with each line:\n",
    "\n",
    "```dockerfile\n",
    "# Broken Dockerfile - Fix the errors\n",
    "from ubuntu\n",
    "\n",
    "RUN apt install python3 python3-pip\n",
    "RUN pip install numpy pandas\n",
    "\n",
    "COPY . .\n",
    "EXPOSE 8888\n",
    "RUN [\"python3\", \"app.py\"]\n",
    "```\n",
    "\n",
    "Write the corrected Dockerfile and list each error with its fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de7fd8",
   "metadata": {},
   "source": [
    "### Question 11 - Writing a Dockerfile to Install Software on a Base Image\n",
    "\n",
    "Create a Dockerfile that starts from an Ubuntu image and installs the following software:\n",
    "\n",
    "- Git version 2.43.0-1ubuntu7.1\n",
    "- SQLite version 3.45.1-1ubuntu2\n",
    "\n",
    "Ensure that you specify the exact versions of the packages by checking their versions after installation. Include commands to clean up the package manager cache after installation to reduce the image size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c602292",
   "metadata": {},
   "source": [
    "#### Please write your anwer here. You can use ```dockerfile to format your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7354df6",
   "metadata": {},
   "source": [
    "### Question 12: Dockerfile for a Jupyter Data Science Environment\n",
    "\n",
    "Create a Dockerfile starting from Ubuntu that:\n",
    "\n",
    "1. Installs Python 3.11 and pip\n",
    "2. Installs `jupyterlab`, `numpy`, `pandas`, `matplotlib`, and `scikit-learn` with specific versions of your choice\n",
    "4. Sets the working directory to `/home/analyst/notebooks`\n",
    "5. Exposes port 8888\n",
    "6. Starts JupyterLab with `--no-browser` and `--ip=0.0.0.0`\n",
    "\n",
    "Clean up apt cache to reduce image size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9c49e",
   "metadata": {},
   "source": [
    "#### Please write your answer here. You can use ```dockerfile to format your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
